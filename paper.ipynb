{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xyDpv79fwnKk",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Задачи, цели работы\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6L1EmpPwrREO",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Цель работы\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Цель данной лабораторной работы заключается в изучении и реализации методов высокого порядка для решения задач оптимизации, таких как Gauss-Newton, Dog Leg, BFGS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ByPXPQJErPMI",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Задачи для достижения указанной цели\n",
    "\n",
    "1. Реализация методов Gauss-Newton и Powell Dog Leg для решения нелинейной регрессии. Сравнение их эффективности с методами, реализованными в предыдущих работах.\n",
    "2. Реализация метода BFGS для минимизации различных функций. Исследование его сходимости и сравнение с другими реализованными методами.\n",
    "3. Реализация и исследование метода L-BFGS, аналогично методу BFGS.\n",
    "4. Подготовка отчёта, содержащего описание реализованных методов, тесты, таблицы и графики для демонстрации результатов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yrRNnwoWrDHf",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Ход работы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZN50cWDDrUhS",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Подготовка среды, определение полезных функций"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g6KyTs-iC1hM",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### В предыдущих сериях:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import profiler\n",
    "import descent\n",
    "import regression\n",
    "import visualization\n",
    "import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (10, 10)\n",
    "np.set_printoptions(precision=2, suppress=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset.get_data()\n",
    "data_weather = dataset.get_data_weather()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate scheduling + Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, f_chunk = dataset.get_linear_loss_func(*data)\n",
    "visualization.linear_demo_2args(\n",
    "    descent.adam_minibatch_descent(\n",
    "        f=f_chunk,\n",
    "        df=descent.numeric_gradient,\n",
    "        x0=np.array([0.0, 0.0]),\n",
    "        decay=descent.exp_decay(1.0, 0.1),\n",
    "        n_epochs=1000,\n",
    "        batch_size=2,\n",
    "        tol=0.09,\n",
    "    ),\n",
    "    f,\n",
    "    *data\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Non-Linear regression function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим датасет средней температуры на протяжении года (от Яндекса):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.visualize_regression([0.0], *data_weather)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видим, данные вполне периодичны. Попробуем их описать синусом:\n",
    "\n",
    "Определим модель как\n",
    "$$M(x, W) = W_1 + W_2 \\cdot x + W_3 \\cdot x^2 + W_4 \\cdot x^3 + W_5 \\cdot \\sin (x \\cdot W_6 + W_7)$$\n",
    "Тогда Loss-функция будет выглядеть как\n",
    "$$\n",
    "f(W) = \\sum_{i\\in{DATA_x}} (DATA_{yi} - M(i))^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, f_chunk = dataset.get_nonlinear_loss_func(*data_weather)\n",
    "points = descent.adam_minibatch_descent(\n",
    "    f=f_chunk,\n",
    "    df=descent.numeric_gradient,\n",
    "    x0=np.array([0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0]),\n",
    "    decay=descent.exp_decay(0.000000001, 0.1),\n",
    "    n_epochs=10,\n",
    "    batch_size=2,\n",
    "    tol=0.09,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.visualize_descent(points, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нетрудно заметить, что у ранее изученных методов (на примере Adam) нет шансов зафитить такую сложную модель."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bDb9OozacWMV",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Gauss-Newton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод Гаусса-Ньютона является итерационным методом решения задачи нелинейной оптимизации. В частности, метод используется для нахождения минимальных значений в наименьших квадратах и полезен в случаях, когда экспериментальным данным можно приблизить нелинейную функцию.\n",
    "\n",
    "Для описания метода предполагается, что у нас есть набор данных $(x_i, y_i)$ для $i=1,2,...,n$ и мы хотим минимизировать невязку между данными и моделью $\\mathbf{F}(\\mathbf{p},x_i)$, где $\\mathbf{p}=[p_1,p_2,...,p_m]^T$ вектор параметров модели. Оптимизируемая функция:\n",
    "\n",
    "$$\n",
    "S(\\mathbf{p}) = \\sum_{i=1}^{n} r_i(\\mathbf{p})^2 = \\sum_{i=1}^{n} (y_i - \\mathbf{F}(\\mathbf{p},x_i))^2\n",
    "$$\n",
    "\n",
    "где $r_i(\\mathbf{p})$ - вектор невязки.\n",
    "\n",
    "Алгоритм следующий:\n",
    "\n",
    "1. Выбрать начальное приближение вектора параметров модели $\\mathbf{p}_{0}$.\n",
    "\n",
    "2. На каждой итерации определить матрицу Якоби $J(\\mathbf{p}_{k})$:\n",
    "\n",
    "$$\n",
    "J(\\mathbf{p}_{k}) = \\begin{bmatrix} \\frac{\\partial r_1(\\mathbf{p})}{\\partial p_1} & \\cdots & \\frac{\\partial r_1(\\mathbf{p})}{\\partial p_m} \\\\ \\vdots & \\ddots & \\vdots\\\\ \\frac{\\partial r_n(\\mathbf{p})}{\\partial p_1} & \\cdots & \\frac{\\partial r_n(\\mathbf{p})}{\\partial p_m} \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "3. Решить линейную систему для коррекции $\\Delta \\mathbf{p}_{k}$:\n",
    "\n",
    "$$\n",
    "(J^{T}(\\mathbf{p}_{k}) J(\\mathbf{p}_{k})) \\Delta \\mathbf{p}_{k} = - J^{T}(\\mathbf{p}_{k}) \\mathbf{r}(\\mathbf{p}_{k})\n",
    "$$\n",
    "\n",
    "4. Обновить значение вектора параметров, используя $\\Delta \\mathbf{p}_{k}$:\n",
    "\n",
    "$$\n",
    "\\mathbf{p}_{k+1} = \\mathbf{p}_{k} + \\Delta \\mathbf{p}_{k}\n",
    "$$\n",
    "\n",
    "5. Проверить условие сходимости (например, по норме $\\Delta \\mathbf{p}_k$):\n",
    "\n",
    "$$\n",
    "\\|\\Delta \\mathbf{p}_{k}\\| < \\text{tol}.\n",
    "$$\n",
    "\n",
    "Если условие выполняется, остановить итерации и вернуть полученный результат $\\mathbf{p}_{k+1}$. В противном случае, повторить процесс, начиная со шага 2.\n",
    "\n",
    "Важно отметить, что скорость сходимости и качество найденного решения сильно зависит от начального приближения $\\mathbf{p}_{0}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Демонстрация на линейной регрессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, f_chunk = dataset.get_linear_loss_func(*data)\n",
    "visualization.linear_demo_2args(\n",
    "    descent.gauss_newton_descent(\n",
    "        x0=np.array([0.0, 0.0]), rsl=f_chunk, grad=descent.numeric_gradient\n",
    "    ),\n",
    "    f,\n",
    "    *data,\n",
    "    \"Время\",\n",
    "    \"Температура, C\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Демонстрация на нелинейной регрессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(descent)\n",
    "reload(dataset)\n",
    "reload(regression)\n",
    "f, f_chunk = dataset.get_nonlinear_loss_func(*data_weather)\n",
    "points = descent.gauss_newton_descent(\n",
    "    x0=np.array([15.0, 0.0, 0.0, 0.0, 8, 1e-7, 2]),\n",
    "    rsl=f_chunk,\n",
    "    grad=descent.numeric_gradient,\n",
    ")\n",
    "print(points)\n",
    "visualization.visualize_descent(points, f)\n",
    "visualization.visualize_regression(\n",
    "    points[-1], *data_weather, \"Время\", \"Температура\", regression.sine_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jjx4ch1lccak",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Powell Dog Leg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Алгоритм Пауэлла — это итеративный алгоритм оптимизации, который решает задачи неограниченной оптимизации.\n",
    "\n",
    "На каждой итерации алгоритм вычисляет шаг Гаусса-Ньютона и шаг наискорейшего спуска. Затем он решает, какой шаг предпринять, исходя из размера шага и текущего радиуса траст региона. Радиус траст региона корректируется на основе отношения фактического уменьшения значения функции к прогнозируемому уменьшению.\n",
    "\n",
    "В алгоритме используются следующие формулы:\n",
    "\n",
    "- Шаг Гаусса-Ньютона: $dp_{gn} = (J^T J)^{-1} J^T r$\n",
    "\n",
    "- Самый крутой шаг спуска: $dp_{sd} = -\\frac{g^T g}{g^T B g} g$, где $g = J^T r$\n",
    "\n",
    "- Направление шага: $dp = \\begin{cases} dp_{gn} & \\text{if } \\|dp_{gn}\\| \\leq \\delta \\\\ dp_{sd} & \\text{if } \\|dp_{sd}\\| \\geq \\delta \\\\ \\alpha dp_{gn} + (1-\\alpha) dp_{sd} & \\text{иначе} \\end{cases}$\n",
    "\n",
    "- Настройка радиуса области доверия: если $\\rho > 0,75$, увеличить $\\delta$, если $\\rho < 0,25$, уменьшить $\\delta$. $\\rho$ определяется как $\\frac{\\|r\\|-\\|r(p-dp)\\|}{\\|dp\\|}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, f_chunk = dataset.get_nonlinear_loss_func(*data_weather)\n",
    "points = descent.powell_dog_leg(\n",
    "    x0=np.array([16.0, 0.0, 0.0, 0.0, 8.5, 1e-7, 2.9]),\n",
    "    rsl=f_chunk,\n",
    "    grad=descent.numeric_gradient,\n",
    ")\n",
    "print(points)\n",
    "visualization.visualize_descent(points, f)\n",
    "visualization.visualize_regression(\n",
    "    points[-1], *data_weather, \"Время\", \"Температура\", regression.sine_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r8bC4Eqmum1F",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9-Au9Midu99q"
   },
   "source": [
    "Методы Gauss-Newton и Powell's Dog Leg являются специализированными методами оптимизации, которые обычно используются для решения задач нелинейной регрессии. Они основаны на идее использования линеаризации модели для упрощения процесса оптимизации.\n",
    "\n",
    "Метод Gauss-Newton\n",
    "\n",
    "Метод Gauss-Newton - это итерационный метод, который использует градиент функции потерь и приближение к матрице Гессе на каждом шаге для обновления параметров модели. Он обычно работает хорошо для задач нелинейной регрессии, где функция потерь достаточно хорошо приближается квадратичной функцией вблизи минимума.\n",
    "\n",
    "*Преимущества:*\n",
    "- Эффективен для задач, где функция потерь хорошо приближается квадратичной функцией.\n",
    "- Прост в реализации.\n",
    "\n",
    "*Недостатки:*\n",
    "- Может быть неэффективен для задач с неквадратичной функцией потерь.\n",
    "- Часто требует больше итераций для сходимости по сравнению с другими методами.\n",
    "\n",
    "Powell's Dog Leg\n",
    "\n",
    "Powell's Dog Leg - это более сложный метод, который пытается сделать компромисс между шагами по направлению градиента и шагами по направлению Гаусса-Ньютона. Он обычно работает лучше, когда функция потерь не является идеально квадратичной.\n",
    "\n",
    "*Преимущества:*\n",
    "- Может справиться с функциями потерь, которые не являются идеально квадратичными.\n",
    "- Часто сходится быстрее, чем Gauss-Newton.\n",
    "\n",
    "*Недостатки:*\n",
    "- Более сложен в реализации.\n",
    "- Может быть нестабильным при плохих начальных приближениях.\n",
    "\n",
    "В целом, оба этих метода имеют преимущества перед классическим градиентным спуском при решении задач нелинейной регрессии, потому что они используют больше информации о функции потерь. Они обычно сходятся быстрее и могут лучше справиться с неквадратичными функциями потерь. Однако они также могут быть более сложными в реализации и могут требовать более тонкой настройки параметров для достижения хороших результатов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ASQ3u3jWRaym",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "## Задание 2. Исследование метода BFGS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Алгоритм Бройдена — Флетчера — Гольдфарба — Шанно (BFGS) --- ещё один из квазиньютоновских методов оптимизации. Как и все методы этой категории, решается задача оптимизации функции $f$ с помощью её разложения в полином второй степени:\n",
    "\n",
    "$f(x_k + p_k) \\approx f(x_k) + \\langle \\nabla f(x_k), p_k \\rangle + \\frac{1}{2}\\langle p_k, B_k p_k \\rangle$\n",
    "\n",
    "Где $B_k = \\nabla^{2} f(x_k)$ --- гессиан функции в точке $x_k$. Поскольку его вычисление обычно очень дорогое, BFGS вычисляет его приближенное решение, после чего получается минимум квадратичной задачи:\n",
    "\n",
    "$p_k = -B_k^{-1}\\nabla f(x_k)$\n",
    "\n",
    "Далее ищется точка, для которой выполняются условия Вольфе и алгоритм \"шагает\" в этом направлении. Также, для удобства, обратный приближенный гессиан обозначается далее $-B_k^{-1} = H_k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве начального приближения гессиана обычно выбирается невырожденная, хорошо обусловленная матрица. Хорошо подходит единичная. Пересчёт осуществляется по формулам:\n",
    "$p_k = -H_k\\nabla f(x_k)$\n",
    "$s_k = x_{k + 1} - x_k = \\alpha \\cdot p_k$\n",
    "$y_k = \\nabla f(x_{k + 1}) - \\nabla f(x_k)$\n",
    "\n",
    "\n",
    "$H_{k + 1} = (E - \\rho_k y_k^{T} s_k) H_k (E - \\rho_k y_k s_k^{T}) + \\rho_k s_k s_k^{T}$, где $\\rho_k = \\frac{1}{y_k s_k}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UYtI_SWaddDn",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Квадратичная функция"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from funcs import f_matias\n",
    "from bfgs import bfgs\n",
    "\n",
    "import descent\n",
    "\n",
    "visualization.visualize_multiple_descent_2args(\n",
    "    {\n",
    "        \"BFGS\": bfgs(\n",
    "            f_matias, descent.numeric_gradient, x_0=np.array((5.0, -5.0)), epochs=1000\n",
    "        ),\n",
    "        \"Gradient descent + dichotomy\": descent.grad_descent_with_dichotomy(\n",
    "            f_matias,\n",
    "            descent.numeric_gradient,\n",
    "            x0=np.array((5.0, -5.0)),\n",
    "            lr=0.1,\n",
    "            tol=0.01,\n",
    "            epoch=1000,\n",
    "        ),\n",
    "    },\n",
    "    f_matias,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WQitydGCdsqA"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ooTBDbW3dfw_",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Линейная регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, f_chunk = dataset.get_linear_loss_func(*data)\n",
    "visualization.linear_multiple_demo_2args(\n",
    "    {\n",
    "        \"BFGS\": bfgs(\n",
    "            f, descent.numeric_gradient, x_0=np.array((5.0, -5.0)), epochs=100\n",
    "        ),\n",
    "        \"Gradient descent + dichotomy\": descent.grad_descent_with_dichotomy(\n",
    "            f,\n",
    "            descent.numeric_gradient,\n",
    "            x0=np.array((5.0, -5.0)),\n",
    "            lr=0.1,\n",
    "            tol=0.01,\n",
    "            epoch=100,\n",
    "        ),\n",
    "        \"Adam + Exp decay\": descent.adam_minibatch_descent(\n",
    "            f=f_chunk,\n",
    "            df=descent.numeric_gradient,\n",
    "            x0=np.array([0.0, 0.0]),\n",
    "            decay=descent.exp_decay(1.0, 0.1),\n",
    "            n_epochs=1000,\n",
    "            batch_size=2,\n",
    "            tol=0.09,\n",
    "        ),\n",
    "    },\n",
    "    f,\n",
    "    *data\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dxTEsjLDdfgT",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Нелинейная регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, f_chunk = dataset.get_nonlinear_loss_func(*data_weather)\n",
    "points = bfgs(\n",
    "    f,\n",
    "    descent.numeric_gradient,\n",
    "    x_0=np.array([17.0, 0.0, 0.0, 0.0, 8.5, 0.0000002, 2.9]),\n",
    "    epochs=10,\n",
    ")\n",
    "print(points)\n",
    "visualization.visualize_descent(points, f)\n",
    "visualization.visualize_regression(\n",
    "    points[-1], *data_weather, \"Время\", \"Температура\", regression.sine_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KVwHFsY5M7Ze",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Выводы\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hyplVmveNANQ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Текстовый вывод"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ulQst9aplKHf",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Бонусное задание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1OA2N1TPlMqE",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Задание 1. Реализация метода L-BFGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Название говорит само за себя: L(imited Memory)-BFGS. Это модификация предыдущего метода оптимизации, позволяющая использовать линейное количество памяти для хранения состояния и проводить итерацию за линейное время $O(md)$, где $m$ --- количество сохранённых в памяти итераций, а $d$ --- размерность векторов.\n",
    "\n",
    "Вместо того, чтобы хранить весь обратный гессиан $H_k$ в памяти, нам достаточно хранить лишь пары $(s_k, y_k)_{k - m \\ldots k}$, после чего мы сможем приближенно вычислить $H_k \\approx \\gamma I$, где $\\gamma = \\frac{s_{k - 1}^{T}y_{k - 1}}{y_{k - 1}^{T}y_{k - 1}}$\n",
    "\n",
    "Таким образом, восстанавливаем направление по алгоритму ниже за $O(md)$:\n",
    "\n",
    "![image](static/lbfgs.svg)\n",
    "\n",
    "Производительности и точность сильно зависят от выбора константы $m$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kVPJNQyJeMg0",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Квадратичная функция"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from funcs import f_matias\n",
    "from bfgs import l_bfgs\n",
    "\n",
    "import descent\n",
    "\n",
    "visualization.visualize_multiple_descent_2args(\n",
    "    {\n",
    "        \"BFGS\": bfgs(\n",
    "            f_matias, descent.numeric_gradient, x_0=np.array((5.0, -5.0)), epochs=1000\n",
    "        ),\n",
    "        \"L-BFGS\": l_bfgs(\n",
    "            f_matias, descent.numeric_gradient, x_0=np.array((5.0, -5.0)), epochs=1000\n",
    "        ),\n",
    "    },\n",
    "    f_matias,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oVuK6MUWeMg0",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Линейная регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, f_chunk = dataset.get_linear_loss_func(*data)\n",
    "visualization.linear_multiple_demo_2args(\n",
    "    {\n",
    "        \"BFGS\": bfgs(\n",
    "            f, descent.numeric_gradient, x_0=np.array((5.0, -5.0)), epochs=100\n",
    "        ),\n",
    "        \"L-BFGS\": l_bfgs(\n",
    "            f, descent.numeric_gradient, x_0=np.array((5.0, -5.0)), epochs=100\n",
    "        ),\n",
    "    },\n",
    "    f,\n",
    "    *data\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fqBGyR6aeMg0",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Нелинейная регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, f_chunk = dataset.get_nonlinear_loss_func(*data_weather)\n",
    "points = l_bfgs(\n",
    "    f,\n",
    "    descent.numeric_gradient,\n",
    "    x_0=np.array([17.0, 0.0, 0.0, 0.0, 8.5, 0.0000002, 2.9]),\n",
    "    epochs=10,\n",
    ")\n",
    "print(points)\n",
    "visualization.visualize_descent(points, f)\n",
    "visualization.visualize_regression(\n",
    "    points[-1], *data_weather, \"Время\", \"Температура\", regression.sine_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DSXLYH4ZeMg0",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Выводы\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8I4HmnbzeMg0",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Текстовый вывод"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cfDjqV_bIQzB",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Заключение\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0YJ__XhfeUF5",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Сравнительный анализ приведённых методов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hvikDeMZeXnN",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Графики"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RoD4C3cjeZBg",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "// Наложенные траектории и восстановленные регрессии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VMz0qc6Recnq",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Использование ресурсов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nLychtbMegQu",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "// Таблица от профайлера"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hAeJgcLLejy_",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Вывод"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3bj6t-sIcjtG"
   },
   "source": [
    "В ходе этой лабораторной работы были изучены и реализованы методы высокого порядка для решения задач оптимизации, включая нелинейную регрессию и минимизацию функций. Были реализованы и исследованы методы Gauss-Newton, Powell Dog Leg, BFGS и L-BFGS.\n",
    "\n",
    "Методы Gauss-Newton и Powell Dog Leg показали свою эффективность при решении задач нелинейной регрессии, особенно когда функция потерь хорошо приближается квадратичной функцией.\n",
    "\n",
    "Метод BFGS эффективно справлялся с оптимизацией квадратичных и линейных функций, но столкнулся с трудностями при оптимизации нелинейной регрессии с синусоидальной моделью. Это подчеркивает важность выбора правильного метода для конкретного типа функции и данных.\n",
    "\n",
    "L-BFGS, с другой стороны, оказался эффективным вариантом для решения задач оптимизации большого размера, где полная матрица Гессе или ее приближение может быть слишком большой для хранения в памяти.\n",
    "\n",
    "В целом, все исследованные методы представляют собой мощные инструменты для оптимизации, но каждый из них имеет свои преимущества и ограничения. Выбор подходящего метода зависит от конкретной задачи, типа функции и данных. Эта работа подчеркнула важность понимания подходящих сценариев применения для каждого из этих методов, а также их потенциальных ограничений."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
