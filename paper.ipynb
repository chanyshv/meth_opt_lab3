{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xyDpv79fwnKk"
   },
   "source": [
    "# Задачи, цели работы\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6L1EmpPwrREO"
   },
   "source": [
    "## Цель работы\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Цель данной лабораторной работы заключается в изучении и реализации методов высокого порядка для решения задач оптимизации, таких как Gauss-Newton, Dog Leg, BFGS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ByPXPQJErPMI"
   },
   "source": [
    "## Задачи для достижения указанной цели\n",
    "\n",
    "1. Реализация методов Gauss-Newton и Powell Dog Leg для решения нелинейной регрессии. Сравнение их эффективности с методами, реализованными в предыдущих работах.\n",
    "2. Реализация метода BFGS для минимизации различных функций. Исследование его сходимости и сравнение с другими реализованными методами.\n",
    "3. Реализация и исследование метода L-BFGS, аналогично методу BFGS.\n",
    "4. Подготовка отчёта, содержащего описание реализованных методов, тесты, таблицы и графики для демонстрации результатов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yrRNnwoWrDHf"
   },
   "source": [
    "# Ход работы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZN50cWDDrUhS"
   },
   "source": [
    "## Подготовка среды, определение полезных функций"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g6KyTs-iC1hM"
   },
   "source": [
    "### В предыдущих сериях:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import profiler\n",
    "import descent\n",
    "import regression\n",
    "import visualization\n",
    "import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (10, 10)\n",
    "np.set_printoptions(precision=2, suppress=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset.get_data()\n",
    "data_weather = dataset.get_data_weather()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate scheduling + Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from funcs import squared\n",
    "\n",
    "f, f_chunk = dataset.get_linear_loss_func(*data)\n",
    "visualization.linear_demo_2args(\n",
    "    descent.adam_minibatch_descent(\n",
    "        f=squared(f_chunk),\n",
    "        df=descent.numeric_gradient,\n",
    "        x0=np.array([0.0, 0.0]),\n",
    "        decay=descent.exp_decay(1.0, 0.1),\n",
    "        n_epochs=1000,\n",
    "        batch_size=2,\n",
    "        tol=0.09,\n",
    "    ),\n",
    "    f,\n",
    "    *data\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Linear regression function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим датасет средней температуры на протяжении года (от Яндекса):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.visualize_regression([0.0], *data_weather)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видим, данные вполне периодичны. Попробуем их описать синусом:\n",
    "\n",
    "Определим модель как\n",
    "$$M(x, W) = W_1 + W_2 \\cdot x + W_3 \\cdot x^2 + W_4 \\cdot x^3 + W_5 \\cdot \\sin (x \\cdot W_6 + W_7)$$\n",
    "Тогда Loss-функция будет выглядеть как\n",
    "$$\n",
    "f(W) = \\sum_{i\\in{DATA_x}} (DATA_{yi} - M(i))^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from funcs import squared\n",
    "\n",
    "f, f_chunk = dataset.get_nonlinear_loss_func(*data_weather)\n",
    "points = descent.adam_minibatch_descent(\n",
    "    f=squared(f_chunk),\n",
    "    df=descent.numeric_gradient,\n",
    "    x0=np.array([0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0]),\n",
    "    decay=descent.exp_decay(0.000000001, 0.1),\n",
    "    n_epochs=10,\n",
    "    batch_size=2,\n",
    "    tol=0.09,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.visualize_descent(points, f)\n",
    "visualization.visualize_regression(\n",
    "    points[-1], *data_weather, \"Время\", \"Температура\", regression.sine_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нетрудно заметить, что у ранее изученных методов (на примере Adam) нет шансов зафитить такую сложную модель."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Rosenbrock function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Определим её следующим образом:\n",
    "$$\n",
    "f(x, y) = (1 - x)^2 + 100(y - x^2)^2\n",
    "$$\n",
    "Это нелинейная функция, будем использовать её минимизировать для демонстрации работы некоторых методов в будущем."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bDb9OozacWMV"
   },
   "source": [
    "### Gauss-Newton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод Гаусса-Ньютона является итерационным методом решения задачи нелинейной оптимизации. В частности, метод используется для нахождения минимальных значений в наименьших квадратах и полезен в случаях, когда экспериментальным данным можно приблизить нелинейную функцию.\n",
    "\n",
    "Для описания метода предполагается, что у нас есть набор данных $(x_i, y_i)$ для $i=1,2,...,n$ и мы хотим минимизировать невязку между данными и моделью $\\mathbf{F}(\\mathbf{p},x_i)$, где $\\mathbf{p}=[p_1,p_2,...,p_m]^T$ вектор параметров модели. Оптимизируемая функция:\n",
    "\n",
    "$$\n",
    "S(\\mathbf{p}) = \\sum_{i=1}^{n} r_i(\\mathbf{p})^2 = \\sum_{i=1}^{n} (y_i - \\mathbf{F}(\\mathbf{p},x_i))^2\n",
    "$$\n",
    "\n",
    "где $r_i(\\mathbf{p})$ - вектор невязки.\n",
    "\n",
    "Алгоритм следующий:\n",
    "\n",
    "1. Выбрать начальное приближение вектора параметров модели $\\mathbf{p}_{0}$.\n",
    "\n",
    "2. На каждой итерации определить матрицу Якоби $J(\\mathbf{p}_{k})$:\n",
    "\n",
    "$$\n",
    "J(\\mathbf{p}_{k}) = \\begin{bmatrix} \\frac{\\partial r_1(\\mathbf{p})}{\\partial p_1} & \\cdots & \\frac{\\partial r_1(\\mathbf{p})}{\\partial p_m} \\\\ \\vdots & \\ddots & \\vdots\\\\ \\frac{\\partial r_n(\\mathbf{p})}{\\partial p_1} & \\cdots & \\frac{\\partial r_n(\\mathbf{p})}{\\partial p_m} \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "3. Решить линейную систему для коррекции $\\Delta \\mathbf{p}_{k}$:\n",
    "\n",
    "$$\n",
    "(J^{T}(\\mathbf{p}_{k}) J(\\mathbf{p}_{k})) \\Delta \\mathbf{p}_{k} = - J^{T}(\\mathbf{p}_{k}) \\mathbf{r}(\\mathbf{p}_{k})\n",
    "$$\n",
    "\n",
    "4. Обновить значение вектора параметров, используя $\\Delta \\mathbf{p}_{k}$:\n",
    "\n",
    "$$\n",
    "\\mathbf{p}_{k+1} = \\mathbf{p}_{k} + \\Delta \\mathbf{p}_{k}\n",
    "$$\n",
    "\n",
    "5. Проверить условие сходимости (например, по норме $\\Delta \\mathbf{p}_k$):\n",
    "\n",
    "$$\n",
    "\\|\\Delta \\mathbf{p}_{k}\\| < \\text{tol}.\n",
    "$$\n",
    "\n",
    "Если условие выполняется, остановить итерации и вернуть полученный результат $\\mathbf{p}_{k+1}$. В противном случае, повторить процесс, начиная со шага 2.\n",
    "\n",
    "Важно отметить, что скорость сходимости и качество найденного решения сильно зависит от начального приближения $\\mathbf{p}_{0}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Демонстрация на линейной регрессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, f_chunk = dataset.get_linear_loss_func(*data)\n",
    "visualization.linear_demo_2args(\n",
    "    descent.gauss_newton_descent(\n",
    "        x0=np.array([0.0, 0.0]), rsl=f_chunk, grad=descent.numeric_gradient\n",
    "    ),\n",
    "    f,\n",
    "    *data,\n",
    "    \"Время\",\n",
    "    \"Температура, C\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Демонстрация на нелинейной регрессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(descent)\n",
    "reload(dataset)\n",
    "reload(regression)\n",
    "f, f_chunk = dataset.get_nonlinear_loss_func(*data_weather)\n",
    "points = descent.gauss_newton_descent(\n",
    "    x0=np.array([15.0, 0.0, 0.0, 0.0, 8, 1e-7, 2]),\n",
    "    rsl=f_chunk,\n",
    "    grad=descent.numeric_gradient,\n",
    ")\n",
    "print(points)\n",
    "visualization.visualize_descent(points, f)\n",
    "visualization.visualize_regression(\n",
    "    points[-1], *data_weather, \"Время\", \"Температура\", regression.sine_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jjx4ch1lccak"
   },
   "source": [
    "### Powell Dog Leg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Алгоритм Пауэлла — это итеративный алгоритм оптимизации, который решает задачи неограниченной оптимизации.\n",
    "\n",
    "На каждой итерации алгоритм вычисляет шаг Гаусса-Ньютона и шаг наискорейшего спуска. Затем он решает, какой шаг предпринять, исходя из размера шага и текущего радиуса траст региона. Радиус траст региона корректируется на основе отношения фактического уменьшения значения функции к прогнозируемому уменьшению.\n",
    "\n",
    "В алгоритме используются следующие формулы:\n",
    "\n",
    "- Шаг Гаусса-Ньютона: $dp_{gn} = (J^T J)^{-1} J^T r$\n",
    "\n",
    "- Самый крутой шаг спуска: $dp_{sd} = -\\frac{g^T g}{g^T B g} g$, где $g = J^T r$\n",
    "\n",
    "- Направление шага: $dp = \\begin{cases} dp_{gn} & \\text{if } \\|dp_{gn}\\| \\leq \\delta \\\\ dp_{sd} & \\text{if } \\|dp_{sd}\\| \\geq \\delta \\\\ \\alpha dp_{gn} + (1-\\alpha) dp_{sd} & \\text{иначе} \\end{cases}$\n",
    "\n",
    "- Настройка радиуса области доверия: если $\\rho > 0,75$, увеличить $\\delta$, если $\\rho < 0,25$, уменьшить $\\delta$. $\\rho$ определяется как $\\frac{\\|r\\|-\\|r(p-dp)\\|}{\\|dp\\|}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, f_chunk = dataset.get_nonlinear_loss_func(*data_weather)\n",
    "points = descent.powell_dog_leg(\n",
    "    x0=np.array([16.0, 0.0, 0.0, 0.0, 8.5, 1e-7, 2.9]),\n",
    "    rsl=f_chunk,\n",
    "    grad=descent.numeric_gradient,\n",
    ")\n",
    "print(points)\n",
    "visualization.visualize_descent(points, f)\n",
    "visualization.visualize_regression(\n",
    "    points[-1], *data_weather, \"Время\", \"Температура\", regression.sine_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r8bC4Eqmum1F"
   },
   "source": [
    "### Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9-Au9Midu99q"
   },
   "source": [
    "Методы Gauss-Newton и Powell's Dog Leg являются специализированными методами оптимизации, которые обычно используются для решения задач нелинейной регрессии. Они основаны на идее использования линеаризации модели для упрощения процесса оптимизации.\n",
    "\n",
    "Метод Gauss-Newton\n",
    "\n",
    "Метод Gauss-Newton - это итерационный метод, который использует градиент функции потерь и приближение к матрице Гессе на каждом шаге для обновления параметров модели. Он обычно работает хорошо для задач нелинейной регрессии, где функция потерь достаточно хорошо приближается квадратичной функцией вблизи минимума.\n",
    "\n",
    "*Преимущества:*\n",
    "- Эффективен для задач, где функция потерь хорошо приближается квадратичной функцией.\n",
    "- Прост в реализации.\n",
    "\n",
    "*Недостатки:*\n",
    "- Может быть неэффективен для задач с неквадратичной функцией потерь.\n",
    "- Часто требует больше итераций для сходимости по сравнению с другими методами.\n",
    "\n",
    "Powell's Dog Leg\n",
    "\n",
    "Powell's Dog Leg - это более сложный метод, который пытается сделать компромисс между шагами по направлению градиента и шагами по направлению Гаусса-Ньютона. Он обычно работает лучше, когда функция потерь не является идеально квадратичной.\n",
    "\n",
    "*Преимущества:*\n",
    "- Может справиться с функциями потерь, которые не являются идеально квадратичными.\n",
    "- Часто сходится быстрее, чем Gauss-Newton.\n",
    "\n",
    "*Недостатки:*\n",
    "- Более сложен в реализации.\n",
    "- Может быть нестабильным при плохих начальных приближениях.\n",
    "\n",
    "В целом, оба этих метода имеют преимущества перед классическим градиентным спуском при решении задач нелинейной регрессии, потому что они используют больше информации о функции потерь. Они обычно сходятся быстрее и могут лучше справиться с неквадратичными функциями потерь. Однако они также могут быть более сложными в реализации и могут требовать более тонкой настройки параметров для достижения хороших результатов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ASQ3u3jWRaym"
   },
   "source": [
    "\n",
    "## Задание 2. Исследование метода BFGS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Алгоритм Бройдена — Флетчера — Гольдфарба — Шанно (BFGS) --- ещё один из квазиньютоновских методов оптимизации. Как и все методы этой категории, решается задача оптимизации функции $f$ с помощью её разложения в полином второй степени:\n",
    "\n",
    "$f(x_k + p_k) \\approx f(x_k) + \\langle \\nabla f(x_k), p_k \\rangle + \\frac{1}{2}\\langle p_k, B_k p_k \\rangle$\n",
    "\n",
    "Где $B_k = \\nabla^{2} f(x_k)$ --- гессиан функции в точке $x_k$. Поскольку его вычисление обычно очень дорогое, BFGS вычисляет его приближенное решение, после чего получается минимум квадратичной задачи:\n",
    "\n",
    "$p_k = -B_k^{-1}\\nabla f(x_k)$\n",
    "\n",
    "Далее ищется точка, для которой выполняются условия Вольфе и алгоритм \"шагает\" в этом направлении. Также, для удобства, обратный приближенный гессиан обозначается далее $-B_k^{-1} = H_k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве начального приближения гессиана обычно выбирается невырожденная, хорошо обусловленная матрица. Хорошо подходит единичная. Пересчёт осуществляется по формулам:\n",
    "$p_k = -H_k\\nabla f(x_k)$\n",
    "$s_k = x_{k + 1} - x_k = \\alpha \\cdot p_k$\n",
    "$y_k = \\nabla f(x_{k + 1}) - \\nabla f(x_k)$\n",
    "\n",
    "\n",
    "$H_{k + 1} = (E - \\rho_k y_k^{T} s_k) H_k (E - \\rho_k y_k s_k^{T}) + \\rho_k s_k s_k^{T}$, где $\\rho_k = \\frac{1}{y_k s_k}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UYtI_SWaddDn"
   },
   "source": [
    "### Квадратичная функция"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from funcs import f_matias\n",
    "from bfgs import bfgs\n",
    "\n",
    "import descent\n",
    "\n",
    "visualization.visualize_multiple_descent_2args(\n",
    "    {\n",
    "        \"BFGS\": bfgs(\n",
    "            f_matias, descent.numeric_gradient, x_0=np.array((5.0, -5.0)), epochs=1000\n",
    "        ),\n",
    "        \"Gradient descent + dichotomy\": descent.grad_descent_with_dichotomy(\n",
    "            f_matias,\n",
    "            descent.numeric_gradient,\n",
    "            x0=np.array((5.0, -5.0)),\n",
    "            lr=0.1,\n",
    "            tol=0.01,\n",
    "            epoch=1000,\n",
    "        ),\n",
    "    },\n",
    "    f_matias,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WQitydGCdsqA"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ooTBDbW3dfw_"
   },
   "source": [
    "### Линейная регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from funcs import squared\n",
    "\n",
    "f, f_chunk = dataset.get_linear_loss_func(*data)\n",
    "visualization.linear_multiple_demo_2args(\n",
    "    {\n",
    "        \"Adam + Exp decay\": descent.adam_minibatch_descent(\n",
    "            f=squared(f_chunk),\n",
    "            df=descent.numeric_gradient,\n",
    "            x0=np.array([5.0, -5.0]),\n",
    "            decay=descent.exp_decay(1.0, 0.1),\n",
    "            n_epochs=1000,\n",
    "            batch_size=2,\n",
    "            tol=0.09,\n",
    "        ),\n",
    "        \"Gradient descent + dichotomy\": descent.grad_descent_with_dichotomy(\n",
    "            f,\n",
    "            descent.numeric_gradient,\n",
    "            x0=np.array((5.0, -5.0)),\n",
    "            lr=0.1,\n",
    "            tol=0.01,\n",
    "            epoch=100,\n",
    "        ),\n",
    "    },\n",
    "    f,\n",
    "    *data,\n",
    "    ((-12, 16), (-12, 12)),\n",
    ")\n",
    "\n",
    "print(\"======\")\n",
    "print(\"BFGS:\")\n",
    "visualization.linear_multiple_demo_2args(\n",
    "    {\n",
    "        \"BFGS\": bfgs(\n",
    "            f, descent.numeric_gradient, x_0=np.array((5.0, -5.0)), epochs=100\n",
    "        ),\n",
    "    },\n",
    "    f,\n",
    "    *data,\n",
    "    ((-62, 12), (-12, 27)),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dxTEsjLDdfgT"
   },
   "source": [
    "### Нелинейная регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, f_chunk = dataset.get_nonlinear_loss_func(*data_weather)\n",
    "points = bfgs(\n",
    "    f,\n",
    "    descent.numeric_gradient,\n",
    "    x_0=np.array([17.0, 0.0, 0.0, 0.0, 8.5, 0.0000002, 2.9]),\n",
    "    epochs=10,\n",
    ")\n",
    "print(points)\n",
    "visualization.visualize_descent(points, f)\n",
    "visualization.visualize_regression(\n",
    "    points[-1], *data_weather, \"Время\", \"Температура\", regression.sine_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Не работает :/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Более простая нелинейная функция (Rosenbrock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from funcs import f_rosenbrock\n",
    "from funcs import f_rosenbrock_chunk\n",
    "from funcs import squared\n",
    "\n",
    "from bfgs import bfgs\n",
    "\n",
    "import descent\n",
    "\n",
    "print(\"Глобальный минимум: [1.0, 1.0]\")\n",
    "visualization.visualize_multiple_descent_2args(\n",
    "    {\n",
    "        \"Gradient descent + dichotomy\": descent.grad_descent_with_dichotomy(\n",
    "            f_rosenbrock,\n",
    "            descent.numeric_gradient,\n",
    "            x0=np.array((-10.0, -10.0)),\n",
    "            lr=0.1,\n",
    "            tol=0.01,\n",
    "            epoch=1000,\n",
    "        ),\n",
    "        \"Adam + Exp decay\": descent.adam_minibatch_descent(\n",
    "            f=squared(f_rosenbrock_chunk()),\n",
    "            df=descent.numeric_gradient,\n",
    "            x0=np.array([-10.0, -10.0]),\n",
    "            decay=descent.constant_lr_decay(0.1),\n",
    "            n_epochs=1000,\n",
    "            batch_size=2,\n",
    "            tol=0.01,\n",
    "        ),\n",
    "        \"Powell Dog Leg\": descent.powell_dog_leg(\n",
    "            x0=np.array([-10.0, -10.0]),\n",
    "            rsl=f_rosenbrock_chunk(),\n",
    "            grad=descent.numeric_gradient,\n",
    "            max_iter=1000,\n",
    "        ),\n",
    "    },\n",
    "    f_rosenbrock,\n",
    "    print_points=True,\n",
    "    visualization_area=((-12, 12), (-17, 12)),\n",
    ")\n",
    "visualization.visualize_multiple_descent_2args(\n",
    "    {\n",
    "        \"BFGS\": bfgs(\n",
    "            f_rosenbrock,\n",
    "            descent.numeric_gradient,\n",
    "            x_0=np.array((-10.0, -10.0)),\n",
    "            epochs=1000,\n",
    "        )\n",
    "    },\n",
    "    f_rosenbrock,\n",
    "    print_points=True,\n",
    "    visualization_area=((-12, 55), (-17, 3750)),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KVwHFsY5M7Ze"
   },
   "source": [
    "### Выводы\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hyplVmveNANQ"
   },
   "source": [
    "Метод BFGS, или метод Бройдена — Флетчера — Гольдфарба — Шанно, является одним из наиболее широко используемых методов оптимизации, применяемых в задачах машинного обучения. Это итеративный метод, который обновляет оценку обратной матрицы Гессиана на каждом шаге и использует это обновление для поиска направления спуска.\n",
    "\n",
    "На основании представленных результатов, можно сделать следующие выводы:\n",
    "\n",
    "1. Метод BFGS показал отличные результаты при работе с квадратичными функциями и линейной регрессией. Это ожидаемо, поскольку BFGS основывается на аппроксимации второго порядка функции потерь, что делает его особенно эффективным для функций, которые хорошо моделируются квадратичными формами.\n",
    "\n",
    "2. В случае нелинейной регрессии с синусоидальной моделью метод BFGS показал невысокую эффективность. Это может быть обусловлено сложностью моделируемой функции, которая могла привести к наличию множества локальных минимумов или плохой сходимости градиентов. Необходимо также учесть, что BFGS делает аппроксимацию второго порядка функции потерь, что может не всегда давать хорошие результаты для функций с сильной нелинейностью.\n",
    "\n",
    "3. Метод BFGS показал отличную работу на функции Розенброка, успешно справившись с задачей и найдя глобальный минимум. Это подчеркивает его эффективность в оптимизации функций, которые могут иметь искаженные или узкие долины, что является характеристикой функции Розенброка.\n",
    "\n",
    "В целом, метод BFGS является мощным инструментом для оптимизации. Однако его эффективность зависит от свойств функции потерь, включая ее форму и сложность. Нелинейность функции может сильно усложнить работу метода, что требует более тщательного подбора начальной точки или параметров метода."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ulQst9aplKHf"
   },
   "source": [
    "# Бонусное задание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1OA2N1TPlMqE"
   },
   "source": [
    "## Задание 1. Реализация метода L-BFGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Название говорит само за себя: L(imited Memory)-BFGS. Это модификация предыдущего метода оптимизации, позволяющая использовать линейное количество памяти для хранения состояния и проводить итерацию за линейное время $O(md)$, где $m$ --- количество сохранённых в памяти итераций, а $d$ --- размерность векторов.\n",
    "\n",
    "Вместо того, чтобы хранить весь обратный гессиан $H_k$ в памяти, нам достаточно хранить лишь пары $(s_k, y_k)_{k - m \\ldots k}$, после чего мы сможем приближенно вычислить $H_k \\approx \\gamma I$, где $\\gamma = \\frac{s_{k - 1}^{T}y_{k - 1}}{y_{k - 1}^{T}y_{k - 1}}$\n",
    "\n",
    "Таким образом, восстанавливаем направление по алгоритму ниже за $O(md)$:\n",
    "\n",
    "![image](static/lbfgs.svg)\n",
    "\n",
    "Производительности и точность сильно зависят от выбора константы $m$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kVPJNQyJeMg0"
   },
   "source": [
    "### Квадратичная функция"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from funcs import f_matias\n",
    "from bfgs import l_bfgs\n",
    "\n",
    "import descent\n",
    "\n",
    "visualization.visualize_multiple_descent_2args(\n",
    "    {\n",
    "        \"BFGS\": bfgs(\n",
    "            f_matias, descent.numeric_gradient, x_0=np.array((5.0, -5.0)), epochs=1000\n",
    "        ),\n",
    "        \"L-BFGS\": l_bfgs(\n",
    "            f_matias, descent.numeric_gradient, x_0=np.array((5.0, -5.0)), epochs=1000\n",
    "        ),\n",
    "    },\n",
    "    f_matias,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oVuK6MUWeMg0"
   },
   "source": [
    "### Линейная регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, f_chunk = dataset.get_linear_loss_func(*data)\n",
    "visualization.linear_multiple_demo_2args(\n",
    "    {\n",
    "        \"BFGS\": bfgs(\n",
    "            f, descent.numeric_gradient, x_0=np.array((5.0, -5.0)), epochs=100\n",
    "        ),\n",
    "        \"L-BFGS\": l_bfgs(\n",
    "            f, descent.numeric_gradient, x_0=np.array((5.0, -5.0)), epochs=100\n",
    "        ),\n",
    "    },\n",
    "    f,\n",
    "    *data\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fqBGyR6aeMg0"
   },
   "source": [
    "### Нелинейная регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, f_chunk = dataset.get_nonlinear_loss_func(*data_weather)\n",
    "points = l_bfgs(\n",
    "    f,\n",
    "    descent.numeric_gradient,\n",
    "    x_0=np.array([17.0, 0.0, 0.0, 0.0, 8.5, 0.0000002, 2.9]),\n",
    "    epochs=10,\n",
    ")\n",
    "print(points)\n",
    "visualization.visualize_descent(points, f)\n",
    "visualization.visualize_regression(\n",
    "    points[-1], *data_weather, \"Время\", \"Температура\", regression.sine_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Более простая нелинейная функция (Rosenbrock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from funcs import f_rosenbrock\n",
    "from funcs import f_rosenbrock_chunk\n",
    "from funcs import squared\n",
    "\n",
    "from bfgs import bfgs\n",
    "\n",
    "import descent\n",
    "\n",
    "print(\"Глобальный минимум: [1.0, 1.0]\")\n",
    "visualization.visualize_multiple_descent_2args(\n",
    "    {\n",
    "        \"BFGS\": bfgs(\n",
    "            f_rosenbrock,\n",
    "            descent.numeric_gradient,\n",
    "            x_0=np.array((-10.0, -10.0)),\n",
    "            epochs=1000,\n",
    "        ),\n",
    "        \"L-BFGS\": l_bfgs(\n",
    "            f_rosenbrock,\n",
    "            descent.numeric_gradient,\n",
    "            x_0=np.array((-10.0, -10.0)),\n",
    "            epochs=1000,\n",
    "        ),\n",
    "        # TODO: добавить версии с l_bfgs с разными параметрами M\n",
    "    },\n",
    "    f_rosenbrock,\n",
    "    print_points=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DSXLYH4ZeMg0"
   },
   "source": [
    "### Выводы\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Метод L-BFGS (Limited-memory Broyden-Fletcher-Goldfarb-Shanno) — это вариация метода BFGS, которая использует значительно меньше памяти. Этот метод был разработан для решения задач оптимизации большого размера, где полный гессиан или его приближение в методе BFGS может быть слишком дорогим для хранения в памяти.\n",
    "\n",
    "Преимущества L-BFGS:\n",
    "\n",
    "1. Эффективность памяти: L-BFGS использует значительно меньше памяти по сравнению с BFGS, что делает его идеальным для больших задач оптимизации.\n",
    "\n",
    "2. Скорость: L-BFGS обычно сходится быстрее, чем BFGS на больших проблемах, потому что он требует меньше памяти и вычислений для каждого шага оптимизации.\n",
    "\n",
    "Недостатки L-BFGS:\n",
    "\n",
    "1. Сложность настройки: L-BFGS имеет дополнительный гиперпараметр — количество хранимых векторов ($m$), которые используются для приближения обратной матрицы Гессе. Настройка этого параметра может быть сложной.\n",
    "\n",
    "2. Неточность: Поскольку L-BFGS использует только ограниченную информацию из прошлых итераций, он может быть менее точным, чем полный BFGS, особенно на небольших задачах оптимизации.\n",
    "\n",
    "В целом, L-BFGS работает лучше, когда решается задача с большим количеством параметров или когда доступ к памяти ограничен. Он может быть менее эффективным на небольших задачах или когда точность является главным приоритетом."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cfDjqV_bIQzB"
   },
   "source": [
    "# Заключение\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hAeJgcLLejy_"
   },
   "source": [
    "## Вывод"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3bj6t-sIcjtG"
   },
   "source": [
    "В ходе этой лабораторной работы были изучены и реализованы методы высокого порядка для решения задач оптимизации, включая нелинейную регрессию и минимизацию функций. Были реализованы и исследованы методы Gauss-Newton, Powell Dog Leg, BFGS и L-BFGS.\n",
    "\n",
    "Методы Gauss-Newton и Powell Dog Leg показали свою эффективность при решении задач нелинейной регрессии, особенно когда функция потерь хорошо приближается квадратичной функцией.\n",
    "\n",
    "Метод BFGS эффективно справлялся с оптимизацией квадратичных и линейных функций, но столкнулся с трудностями при оптимизации нелинейной регрессии с синусоидальной моделью. Это подчеркивает важность выбора правильного метода для конкретного типа функции и данных.\n",
    "\n",
    "L-BFGS, с другой стороны, оказался эффективным вариантом для решения задач оптимизации большого размера, где полная матрица Гессе или ее приближение может быть слишком большой для хранения в памяти.\n",
    "\n",
    "В целом, все исследованные методы представляют собой мощные инструменты для оптимизации, но каждый из них имеет свои преимущества и ограничения. Выбор подходящего метода зависит от конкретной задачи, типа функции и данных. Эта работа подчеркнула важность понимания подходящих сценариев применения для каждого из этих методов, а также их потенциальных ограничений."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
