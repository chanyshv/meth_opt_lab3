{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xyDpv79fwnKk",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Задачи, цели работы\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6L1EmpPwrREO",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Цель работы\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Цель данной лабораторной работы заключается в изучении и реализации методов высокого порядка для решения задач оптимизации, таких как Gauss-Newton, Dog Leg, BFGS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ByPXPQJErPMI",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Задачи для достижения указанной цели\n",
    "\n",
    "1. Реализация методов Gauss-Newton и Powell Dog Leg для решения нелинейной регрессии. Сравнение их эффективности с методами, реализованными в предыдущих работах.\n",
    "2. Реализация метода BFGS для минимизации различных функций. Исследование его сходимости и сравнение с другими реализованными методами.\n",
    "3. Реализация и исследование метода L-BFGS, аналогично методу BFGS.\n",
    "4. Подготовка отчёта, содержащего описание реализованных методов, тесты, таблицы и графики для демонстрации результатов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yrRNnwoWrDHf",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Ход работы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZN50cWDDrUhS",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Подготовка среды, определение полезных функций"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g6KyTs-iC1hM",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### В предыдущих сериях:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import profiler\n",
    "import descent\n",
    "import regression\n",
    "import visualization\n",
    "import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (10, 10)\n",
    "np.set_printoptions(precision=2, suppress=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset.get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Non-Linear regression function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Learning rate scheduling + Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, f_chunk = dataset.get_linear_loss_func(*data)\n",
    "visualization.linear_demo_2args(\n",
    "    descent.adam_minibatch_descent(\n",
    "        f=f_chunk,\n",
    "        df=descent.numeric_gradient,\n",
    "        x0=np.array([0.0, 0.0]),\n",
    "        decay=descent.exp_decay(1.0, 0.1),\n",
    "        n_epochs=1000,\n",
    "        batch_size=2,\n",
    "        tol=0.09,\n",
    "    ),\n",
    "    f,\n",
    "    *data\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bDb9OozacWMV",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Gauss-Newton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод Гаусса-Ньютона является итерационным методом решения задачи нелинейной оптимизации. В частности, метод используется для нахождения минимальных значений в наименьших квадратах и полезен в случаях, когда экспериментальным данным можно приблизить нелинейную функцию.\n",
    "\n",
    "Для описания метода предполагается, что у нас есть набор данных $(x_i, y_i)$ для $i=1,2,...,n$ и мы хотим минимизировать невязку между данными и моделью $\\mathbf{F}(\\mathbf{p},x_i)$, где $\\mathbf{p}=[p_1,p_2,...,p_m]^T$ вектор параметров модели. Оптимизируемая функция:\n",
    "\n",
    "$$\n",
    "S(\\mathbf{p}) = \\sum_{i=1}^{n} r_i(\\mathbf{p})^2 = \\sum_{i=1}^{n} (y_i - \\mathbf{F}(\\mathbf{p},x_i))^2\n",
    "$$\n",
    "\n",
    "где $r_i(\\mathbf{p})$ - вектор невязки.\n",
    "\n",
    "Алгоритм следующий:\n",
    "\n",
    "1. Выбрать начальное приближение вектора параметров модели $\\mathbf{p}_{0}$.\n",
    "\n",
    "2. На каждой итерации определить матрицу Якоби $J(\\mathbf{p}_{k})$:\n",
    "\n",
    "$$\n",
    "J(\\mathbf{p}_{k}) = \\begin{bmatrix} \\frac{\\partial r_1(\\mathbf{p})}{\\partial p_1} & \\cdots & \\frac{\\partial r_1(\\mathbf{p})}{\\partial p_m} \\\\ \\vdots & \\ddots & \\vdots\\\\ \\frac{\\partial r_n(\\mathbf{p})}{\\partial p_1} & \\cdots & \\frac{\\partial r_n(\\mathbf{p})}{\\partial p_m} \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "3. Решить линейную систему для коррекции $\\Delta \\mathbf{p}_{k}$:\n",
    "\n",
    "$$\n",
    "(J^{T}(\\mathbf{p}_{k}) J(\\mathbf{p}_{k})) \\Delta \\mathbf{p}_{k} = - J^{T}(\\mathbf{p}_{k}) \\mathbf{r}(\\mathbf{p}_{k})\n",
    "$$\n",
    "\n",
    "4. Обновить значение вектора параметров, используя $\\Delta \\mathbf{p}_{k}$:\n",
    "\n",
    "$$\n",
    "\\mathbf{p}_{k+1} = \\mathbf{p}_{k} + \\Delta \\mathbf{p}_{k}\n",
    "$$\n",
    "\n",
    "5. Проверить условие сходимости (например, по норме $\\Delta \\mathbf{p}_k$):\n",
    "\n",
    "$$\n",
    "\\|\\Delta \\mathbf{p}_{k}\\| < \\text{tol}.\n",
    "$$\n",
    "\n",
    "Если условие выполняется, остановить итерации и вернуть полученный результат $\\mathbf{p}_{k+1}$. В противном случае, повторить процесс, начиная со шага 2.\n",
    "\n",
    "Важно отметить, что скорость сходимости и качество найденного решения сильно зависит от начального приближения $\\mathbf{p}_{0}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jjx4ch1lccak",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Powell Dog Leg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<TeX описание>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r8bC4Eqmum1F",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9-Au9Midu99q",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "// текстовый вывод"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ASQ3u3jWRaym",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "## Задание 2. Исследование метода BFGS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Алгоритм Бройдена — Флетчера — Гольдфарба — Шанно (BFGS) --- ещё один из квазиньютоновских методов оптимизации. Как и все методы этой категории, решается задача оптимизации функции $f$ с помощью её разложения в полином второй степени:\n",
    "\n",
    "$f(x_k + p_k) \\approx f(x_k) + \\langle \\nabla f(x_k), p_k \\rangle + \\frac{1}{2}\\langle p_k, B_k p_k \\rangle$\n",
    "\n",
    "Где $B_k = \\nabla^{2} f(x_k)$ --- гессиан функции в точке $x_k$. Поскольку его вычисление обычно очень дорогое, BFGS вычисляет его приближенное решение, после чего получается минимум квадратичной задачи:\n",
    "\n",
    "$p_k = -B_k^{-1}\\nabla f(x_k)$\n",
    "\n",
    "Далее ищется точка, для которой выполняются условия Вольфе и алгоритм \"шагает\" в этом направлении. Также, для удобства, обратный приближенный гессиан обозначается далее $-B_k^{-1} = H_k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "В качестве начального приближения гессиана обычно выбирается невырожденная, хорошо обусловленная матрица. Хорошо подходит единичная. Пересчёт осуществляется по формулам:\n",
    "$p_k = -H_k\\nabla f(x_k)$\n",
    "$s_k = x_{k + 1} - x_k = \\alpha \\cdot p_k$\n",
    "$y_k = \\nabla f(x_{k + 1}) - \\nabla f(x_k)$\n",
    "\n",
    "\n",
    "$H_{k + 1} = (E - \\rho_k y_k^{T} s_k) H_k (E - \\rho_k y_k s_k^{T}) + \\rho_k s_k s_k^{T}$, где $\\rho_k = \\frac{1}{y_k s_k}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UYtI_SWaddDn",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Квадратичная функция"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from funcs import f_matias\n",
    "from bfgs import bfgs\n",
    "\n",
    "import descent\n",
    "\n",
    "visualization.visualize_multiple_descent_2args(\n",
    "    {\n",
    "        \"BFGS\": bfgs(\n",
    "            f_matias, descent.numeric_gradient, x_0=np.array((5.0, -5.0)), epochs=1000\n",
    "        ),\n",
    "        \"Gradient descent + dichotomy\": descent.grad_descent_with_dichotomy(\n",
    "            f_matias,\n",
    "            descent.numeric_gradient,\n",
    "            x0=np.array((5.0, -5.0)),\n",
    "            lr=0.1,\n",
    "            tol=0.01,\n",
    "            epoch=1000,\n",
    "        ),\n",
    "    },\n",
    "    f_matias,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WQitydGCdsqA"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ooTBDbW3dfw_",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Линейная регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, f_chunk = dataset.get_linear_loss_func(*data)\n",
    "visualization.linear_multiple_demo_2args(\n",
    "    {\n",
    "        \"BFGS\": bfgs(\n",
    "            f, descent.numeric_gradient, x_0=np.array((5.0, -5.0)), epochs=100\n",
    "        ),\n",
    "        \"Gradient descent + dichotomy\": descent.grad_descent_with_dichotomy(\n",
    "            f,\n",
    "            descent.numeric_gradient,\n",
    "            x0=np.array((5.0, -5.0)),\n",
    "            lr=0.1,\n",
    "            tol=0.01,\n",
    "            epoch=100,\n",
    "        ),\n",
    "        \"Adam + Exp decay\": descent.adam_minibatch_descent(\n",
    "            f=f_chunk,\n",
    "            df=descent.numeric_gradient,\n",
    "            x0=np.array([0.0, 0.0]),\n",
    "            decay=descent.exp_decay(1.0, 0.1),\n",
    "            n_epochs=1000,\n",
    "            batch_size=2,\n",
    "            tol=0.09,\n",
    "        ),\n",
    "    },\n",
    "    f,\n",
    "    *data\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dxTEsjLDdfgT",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Нелинейная регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m9ZzvtnMdxl6",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "// Демонстрация + наложенная траектория прошлых методов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y7ZhmX3kdj-4",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### // ?????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WHqRyvfgdyvA",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "// Демонстрация + наложенная траектория прошлых методов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KVwHFsY5M7Ze",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Выводы\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hyplVmveNANQ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Текстовый вывод"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ulQst9aplKHf",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Бонусное задание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1OA2N1TPlMqE",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Задание 1. Реализация метода L-BFGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Название говорит само за себя: L(imited Memory)-BFGS. Это модификация предыдущего метода оптимизации, позволяющая использовать линейное количество памяти для хранения состояния и проводить итерацию за линейное время $O(md)$, где $m$ --- количество сохранённых в памяти итераций, а $d$ --- размерность векторов.\n",
    "\n",
    "Вместо того, чтобы хранить весь обратный гессиан $H_k$ в памяти, нам достаточно хранить лишь пары $(s_k, y_k)_{k - m \\ldots k}$, после чего мы сможем приближенно вычислить $H_k \\approx \\gamma I$, где $\\gamma = \\frac{s_{k - 1}^{T}y_{k - 1}}{y_{k - 1}^{T}y_{k - 1}}$\n",
    "\n",
    "Таким образом, восстанавливаем направление по алгоритму ниже за $O(md)$:\n",
    "\n",
    "![image](static/lbfgs.svg)\n",
    "\n",
    "Производительности и точность сильно зависят от выбора константы $m$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kVPJNQyJeMg0",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Квадратичная функция"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from funcs import f_matias\n",
    "from bfgs import l_bfgs\n",
    "\n",
    "import descent\n",
    "\n",
    "visualization.visualize_multiple_descent_2args(\n",
    "    {\n",
    "        \"BFGS\": bfgs(\n",
    "            f_matias, descent.numeric_gradient, x_0=np.array((5.0, -5.0)), epochs=1000\n",
    "        ),\n",
    "        \"L-BFGS\": l_bfgs(\n",
    "            f_matias, descent.numeric_gradient, x_0=np.array((5.0, -5.0)), epochs=1000\n",
    "        ),\n",
    "    },\n",
    "    f_matias,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oVuK6MUWeMg0",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Линейная регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, f_chunk = dataset.get_linear_loss_func(*data)\n",
    "visualization.linear_multiple_demo_2args(\n",
    "    {\n",
    "        \"BFGS\": bfgs(\n",
    "            f, descent.numeric_gradient, x_0=np.array((5.0, -5.0)), epochs=100\n",
    "        ),\n",
    "        \"L-BFGS\": l_bfgs(\n",
    "            f, descent.numeric_gradient, x_0=np.array((5.0, -5.0)), epochs=100\n",
    "        ),\n",
    "    },\n",
    "    f,\n",
    "    *data\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fqBGyR6aeMg0",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Нелинейная регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jfNPaI0geMg0",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "// Демонстрация + наложенная траектория прошлых методов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8nZf4Fj5eMg0",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### // ?????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wGnxxH7feMg0",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "// Демонстрация + наложенная траектория прошлых методов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DSXLYH4ZeMg0",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Выводы\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8I4HmnbzeMg0",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Текстовый вывод"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cfDjqV_bIQzB",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Заключение\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0YJ__XhfeUF5",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Сравнительный анализ приведённых методов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hvikDeMZeXnN",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Графики"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RoD4C3cjeZBg",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "// Наложенные траектории и восстановленные регрессии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VMz0qc6Recnq",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Использование ресурсов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nLychtbMegQu",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "// Таблица от профайлера"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hAeJgcLLejy_",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Вывод"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3bj6t-sIcjtG",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "// Текстовый финал."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
